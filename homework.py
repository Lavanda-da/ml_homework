# -*- coding: utf-8 -*-
"""homework.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BMlWYgx9EybpWrG_GtQ6WBCJM2lzHvGJ

# Домашняя работа по регуляризации и оптимизации

Ниже приводится корпус данных с двумя метками: 1 и -1. К данным применяется линейная модель классификации:

$f(x, \theta) = x_1 \theta_1 + x_2 \theta_2 + \theta_3.$

Предлагается подобрать параметры $\theta$ минимизируя следующую функцию ошибки:

$\mathcal{L}(\theta) = 0.1 \|\theta\|^2 + \frac{1}{N}\sum\limits_{i=1}^N \max(0, 1 - y_i f(x_i, \theta)).$

Для оптимизации предлагается использовать метод градиентного спуска с 1000 шагами размера $0.1$ из начальной точки $(1, 1, 0)$.
"""

import numpy as np
import yaml

X = np.array([
    [0, 1, 1],
    [1, 1, 1],
    [1, 0, 1],
    [-0.5, 0.5, 1],
    [0, -0.5, 1]
])

y = np.array([1, 1, 1, -1, -1])

theta0 = np.array([1.0, 1.0, 0.0])

lr = 0.1

def f(X, theta):
    theta = np.asarray(theta)
    return (X * theta).sum(axis=-1)

def loss(X, y, theta):
    theta = np.asarray(theta)
    norm = (theta ** 2).sum()
    deltas = y * f(X, theta)
    return 0.1 * norm + np.mean(np.maximum(0, 1 - deltas))

print("Prediction:", f(X, theta0))
print("Loss:", loss(X, y, theta0))

"""На кажой итерации будем оценивать и минимизировать ошибку по одному набору.

Напишем функцию для вычисления производной для этого случая.
"""

def loss_derivative(X, y, theta):
    theta = np.asarray(theta)
    norm = 0.2 * theta
    delta = 0
    for i in range(len(X)):
        deltas = y[i] * f(X[i], theta)
        if 1 - deltas >= 0:
            delta -= y[i] * X[i]
    delta = delta / len(X)
    norm += delta
    return norm

# Ваш код оптимизации.
theta = theta0
for i in range(1000):
    theta = theta - lr * loss_derivative(X, y, theta)

print("Prediction:", f(X, theta))
print("Loss:", loss(X, y, theta))

with open("submission.yaml", "w") as fp:
    yaml.safe_dump({"tasks": [{"task1": {"answer": theta.tolist()}}]}, fp)